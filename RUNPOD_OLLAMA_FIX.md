# ğŸ”§ RunPod Ollama "Bad Gateway" HatasÄ± Ã‡Ã¶zÃ¼mÃ¼

## Sorun
RunPod endpoint'ine baÄŸlanÄ±rken "Bad gateway" (502) hatasÄ± alÄ±yorsunuz. Bu, Ollama servisinin pod'da Ã§alÄ±ÅŸmadÄ±ÄŸÄ± anlamÄ±na gelir.

## Ã‡Ã¶zÃ¼m AdÄ±mlarÄ±

### 1. RunPod Pod'unda Ollama'yÄ± BaÅŸlatÄ±n

RunPod web terminal'inden veya SSH ile pod'a baÄŸlanÄ±n ve ÅŸu komutlarÄ± Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
# Ã–nce sÃ¼reci temizleyin
pkill ollama

# DeÄŸiÅŸkeni set edin (0.0.0.0 tÃ¼m aÄŸlardan eriÅŸim iÃ§in)
export OLLAMA_HOST=0.0.0.0:11434

# Arka planda baÅŸlatÄ±n
ollama serve &
```

### 2. Ollama'nÄ±n Ã‡alÄ±ÅŸtÄ±ÄŸÄ±nÄ± Kontrol Edin

```bash
# Ollama'nÄ±n Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± kontrol edin
ps aux | grep ollama

# Port 11434'Ã¼n dinlendiÄŸini kontrol edin
netstat -tuln | grep 11434
# veya
ss -tuln | grep 11434
```

### 3. Model YÃ¼klÃ¼ mÃ¼ Kontrol Edin

```bash
# YÃ¼klÃ¼ modelleri listeleyin
ollama list

# EÄŸer model yoksa, yÃ¼kleyin
ollama pull llama2:7b
# veya
ollama pull llama2:13b
# veya
ollama pull qwen2.5:32b
```

### 4. Ollama'yÄ± Test Edin

```bash
# Ollama API'sini test edin
curl http://localhost:11434/api/tags

# Model Ã§alÄ±ÅŸÄ±yor mu test edin
curl http://localhost:11434/api/generate -d '{
  "model": "llama2:7b",
  "prompt": "Hello",
  "stream": false
}'
```

### 5. Backend'i Yeniden BaÅŸlatÄ±n

Local bilgisayarÄ±nÄ±zda:

```bash
docker-compose restart backend
```

### 6. Backend LoglarÄ±nÄ± Kontrol Edin

```bash
docker-compose logs backend --tail 50
```

## Alternatif: RunPod Template KullanÄ±n

EÄŸer manuel baÅŸlatma sorun Ã§Ä±karÄ±yorsa, RunPod'da "Ollama" template'ini kullanarak yeni bir pod oluÅŸturun:

1. RunPod'da "Deploy" butonuna tÄ±klayÄ±n
2. Template: "Ollama" seÃ§in
3. GPU: RTX 3090 veya RTX 5090 seÃ§in
4. "Deploy" butonuna tÄ±klayÄ±n

Bu template otomatik olarak Ollama'yÄ± baÅŸlatÄ±r ve doÄŸru ÅŸekilde yapÄ±landÄ±rÄ±r.

## Ã–nemli Notlar

1. **OLLAMA_HOST DeÄŸiÅŸkeni**: `0.0.0.0:11434` olarak ayarlanmalÄ± (sadece `0.0.0.0` deÄŸil, port da belirtilmeli)

2. **HTTPS vs HTTP**: RunPod proxy HTTPS kullanÄ±r, ama Ollama HTTP Ã¼zerinden Ã§alÄ±ÅŸÄ±r. RunPod proxy otomatik olarak HTTPS'yi HTTP'ye Ã§evirir.

3. **Model YÃ¼kleme**: Ä°lk model yÃ¼kleme 5-15 dakika sÃ¼rebilir (model boyutuna gÃ¶re).

4. **Pod Yeniden BaÅŸlatma**: Pod yeniden baÅŸlatÄ±ldÄ±ÄŸÄ±nda Ollama'yÄ± tekrar baÅŸlatmanÄ±z gerekebilir. Bunu otomatikleÅŸtirmek iÃ§in pod'un startup script'ine ekleyebilirsiniz.

## Sorun Giderme

### Ollama baÅŸlamÄ±yor
- Pod'un yeterli RAM'e sahip olduÄŸundan emin olun
- Disk alanÄ±nÄ± kontrol edin: `df -h`
- LoglarÄ± kontrol edin: `journalctl -u ollama` (eÄŸer systemd kullanÄ±yorsa)

### Model yÃ¼klenmiyor
- Disk alanÄ±nÄ± kontrol edin
- Daha kÃ¼Ã§Ã¼k bir model deneyin (7B â†’ 13B â†’ 32B)
- Ä°nternet baÄŸlantÄ±sÄ±nÄ± kontrol edin

### Backend baÄŸlanamÄ±yor
- RunPod endpoint URL'ini kontrol edin
- Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± kontrol edin
- Backend loglarÄ±nÄ± kontrol edin: `docker-compose logs backend`


