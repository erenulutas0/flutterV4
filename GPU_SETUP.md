# ğŸš€ GPU Kiralama ile HÄ±zlandÄ±rma Rehberi

## Neden GPU Kiralama?

CPU'da Ã§alÄ±ÅŸan LLM modelleri Ã§ok yavaÅŸtÄ±r. GPU ile:
- **10-100x daha hÄ±zlÄ±** yanÄ±t sÃ¼releri
- Daha bÃ¼yÃ¼k modeller kullanabilme (32B, 70B)
- Daha iyi kalite ve tutarlÄ±lÄ±k

## SeÃ§enekler ve KarÅŸÄ±laÅŸtÄ±rma

### 1. RunPod (Ã–nerilen) â­
**Avantajlar:**
- Ollama'yÄ± hazÄ±r template ile Ã§alÄ±ÅŸtÄ±rma
- GPU'lu pod'larda otomatik kurulum
- Kolay API endpoint yapÄ±landÄ±rmasÄ±
- Ä°yi dokÃ¼mantasyon

**Fiyat:** ~$0.20-0.50/saat (RTX 3090/4090)

**Kurulum:**
1. https://www.runpod.io/ adresine kaydolun
2. "Templates" â†’ "Ollama" template'ini seÃ§in
3. GPU seÃ§in (RTX 3090 veya 4090 Ã¶nerilir)
4. Pod'u baÅŸlatÄ±n
5. API endpoint'i alÄ±n (Ã¶rn: `https://xxxxx-xxxxx.runpod.net`)

### 2. io.net
**Avantajlar:**
- Decentralized GPU network
- Esnek fiyatlandÄ±rma
- API endpoint saÄŸlar

**Fiyat:** DeÄŸiÅŸken (genellikle uygun)

**Kurulum:**
1. https://cloud.io.net/ adresine kaydolun
2. GPU instance oluÅŸturun
3. Ollama'yÄ± kurun ve Ã§alÄ±ÅŸtÄ±rÄ±n
4. Public endpoint oluÅŸturun

### 3. Vast.ai
**Avantajlar:**
- En ucuz seÃ§enek
- Ã‡ok sayÄ±da GPU seÃ§eneÄŸi

**Dezavantajlar:**
- Manuel kurulum gerekir
- Daha az stabil olabilir

**Fiyat:** ~$0.10-0.30/saat

### 4. Together.ai (En Kolay - API Servisi)
**Avantajlar:**
- HiÃ§ kurulum gerekmez
- Direkt API kullanÄ±mÄ±
- Ã‡ok hÄ±zlÄ± ve gÃ¼venilir

**Dezavantajlar:**
- Ollama yerine kendi API'lerini kullanÄ±r
- Kod deÄŸiÅŸikliÄŸi gerekir
- Biraz daha pahalÄ± olabilir

**Fiyat:** Pay-as-you-go (~$0.0001-0.001 per 1K tokens)

## RunPod ile Entegrasyon (Ã–rnek)

### AdÄ±m 1: RunPod'da Ollama Pod'u OluÅŸtur

1. RunPod'a giriÅŸ yapÄ±n
2. "Pods" â†’ "Deploy" â†’ "Community Cloud"
3. Template: "Ollama" seÃ§in
4. GPU: RTX 3090 veya 4090 seÃ§in
5. "Deploy" butonuna tÄ±klayÄ±n

### AdÄ±m 2: Model YÃ¼kleme

Pod baÅŸladÄ±ktan sonra, terminal'de:

```bash
# Pod'a baÄŸlan
# RunPod web UI'dan terminal'e eriÅŸin

# Model yÃ¼kle (7B model hÄ±zlÄ±, 13B daha iyi kalite)
ollama pull llama2:13b

# Veya daha bÃ¼yÃ¼k model (daha yavaÅŸ ama Ã§ok daha iyi)
ollama pull qwen2.5:32b
```

### AdÄ±m 3: API Endpoint'i Al

RunPod pod'unuzun public endpoint'ini alÄ±n:
- Ã–rnek: `https://xxxxx-xxxxx-5000.proxy.runpod.net`

### AdÄ±m 4: Backend YapÄ±landÄ±rmasÄ±

`docker-compose.yml` dosyasÄ±nÄ± gÃ¼ncelleyin:

```yaml
# Ollama (GPU kiralama servisi)
LANGCHAIN4J_OLLAMA_CHAT_MODEL_BASE_URL: https://xxxxx-xxxxx-5000.proxy.runpod.net
LANGCHAIN4J_OLLAMA_CHAT_MODEL_MODEL_NAME: llama2:13b  # veya qwen2.5:32b
```

### AdÄ±m 5: Backend'i Yeniden BaÅŸlat

```bash
docker-compose restart backend
```

## Together.ai ile Entegrasyon (Alternatif)

EÄŸer Together.ai kullanmak isterseniz, LangChain4j yerine direkt HTTP client kullanmanÄ±z gerekir.

### Avantajlar:
- HiÃ§ sunucu yÃ¶netimi yok
- Ã‡ok hÄ±zlÄ± (managed infrastructure)
- Kolay Ã¶lÃ§eklenebilir

### Kod DeÄŸiÅŸikliÄŸi Gerekir:
- `ChatbotService` interface'ini deÄŸiÅŸtirmeniz gerekir
- Together.ai API'sini kullanacak ÅŸekilde gÃ¼ncelleme

## Maliyet Tahmini

### RunPod (RTX 3090):
- Saatlik: ~$0.30
- AylÄ±k (8 saat/gÃ¼n): ~$72
- AylÄ±k (24/7): ~$216

### io.net:
- DeÄŸiÅŸken, genellikle RunPod'dan biraz daha ucuz

### Vast.ai:
- Saatlik: ~$0.15-0.25
- AylÄ±k (8 saat/gÃ¼n): ~$36-60

### Together.ai:
- Pay-as-you-go
- 1M token ~$0.50-2.00 (modele gÃ¶re)
- KullanÄ±m bazlÄ±, boÅŸta maliyet yok

## Ã–neri

**BaÅŸlangÄ±Ã§ iÃ§in:** RunPod (kolay kurulum, iyi dokÃ¼mantasyon)
**Uzun vadeli:** Together.ai (yÃ¶netim yok, Ã¶lÃ§eklenebilir)
**BÃ¼tÃ§e odaklÄ±:** Vast.ai (en ucuz, manuel kurulum)

## Test Etme

GPU'lu servisi kullanmaya baÅŸladÄ±ktan sonra:

1. Backend loglarÄ±nÄ± kontrol edin:
```bash
docker-compose logs backend -f
```

2. CÃ¼mle Ã¼retim sÃ¼resini Ã¶lÃ§Ã¼n (Ã¶ncesi vs sonrasÄ±)

3. Model kalitesini deÄŸerlendirin

## Notlar

- GPU servisleri genellikle **idle timeout**'a sahiptir (kullanÄ±lmadÄ±ÄŸÄ±nda kapanÄ±r)
- RunPod'da "Network Volume" kullanarak model dosyalarÄ±nÄ± kalÄ±cÄ± hale getirebilirsiniz
- API endpoint'leri genellikle HTTPS gerektirir (CORS ayarlarÄ±na dikkat)


